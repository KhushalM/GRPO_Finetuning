{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GRPO Fine-tuning for Qwen2-VL-2B-Instruct\\n\n",
                "## First Principles Explanations using Custom Reward Function\\n\n",
                "\\n\n",
                "This notebook implements GRPO (Group Relative Policy Optimization) training for fine-tuning Qwen2-VL-2B-Instruct to generate better first principles explanations using a custom reward function.\\n\n",
                "\\n\n",
                "### Key Features:\\n\n",
                "- Custom reward function based on Feynman's teaching principles\\n\n",
                "- Multi-component reward evaluation (analogies, clarity, engagement, etc.)\\n\n",
                "- LoRA fine-tuning for efficient training\\n\n",
                "- Weights & Biases integration for monitoring\\n\n",
                "- Optimized for Google Colab"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "plaintext"
                }
            },
            "outputs": [],
            "source": [
                "## üöÄ Setup and Installation\n",
                "First, let's install all required packages for Colab environment\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
                "!pip install transformers datasets accelerate peft bitsandbytes\n",
                "!pip install trl wandb huggingface_hub\n",
                "!pip install nltk textstat\n",
                "!pip install qwen-vl-utils\n",
                "\n",
                "# Download NLTK data\n",
                "import nltk\n",
                "nltk.download('punkt')\n",
                "nltk.download('stopwords')\n",
                "\n",
                "print(\"‚úÖ All packages installed successfully!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## üîß Mount Google Drive and Setup Environment\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Set up working directory\n",
                "import os\n",
                "os.chdir('/content')\n",
                "\n",
                "# Check GPU availability\n",
                "import torch\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## üîë Authentication Setup\n",
                "Login to Hugging Face and Weights & Biases\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "import wandb\n",
                "\n",
                "# Login to Hugging Face\n",
                "print(\"Please enter your Hugging Face token:\")\n",
                "login()\n",
                "\n",
                "# Login to Weights & Biases\n",
                "print(\"Please enter your W&B API key:\")\n",
                "wandb.login()\n",
                "\n",
                "print(\"‚úÖ Authentication completed!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## üéØ Custom Reward Function Implementation\n",
                "This reward function evaluates first principles explanations based on multiple criteria\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re \n",
                "import math\n",
                "import nltk\n",
                "from typing import Dict, List, Tuple, Any\n",
                "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
                "from collections import Counter\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
                "import torch\n",
                "import random\n",
                "\n",
                "class FirstPrinciplesRewardFunction:\n",
                "    \"\"\"\n",
                "    Comprehensive reward function for evaluating first principles explanations\n",
                "    following the Feynman method: simple analogies, step-by-step reasoning, \n",
                "    engaging storytelling, and fundamental understanding.\n",
                "    \"\"\"\n",
                "    def __init__(self, model_name: str = \"Qwen/Qwen2-VL-2B-Instruct\"):\n",
                "        self.weights = {\n",
                "            'analogy_quality': 0.20,\n",
                "            'step_by_step_reasoning': 0.15,\n",
                "            'fundamental_understanding': 0.20,\n",
                "            'overall_engagement': 0.15,\n",
                "            'clarity': 0.15,\n",
                "            'completeness': 0.10,\n",
                "            'avoid_jargon': 0.05\n",
                "        }\n",
                "        \n",
                "        try:\n",
                "            self.sentiment_analyzer = pipeline(\n",
                "                \"sentiment-analysis\",\n",
                "                model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
                "                device=0 if torch.cuda.is_available() else -1\n",
                "            )\n",
                "        except Exception as e:\n",
                "            self.sentiment_analyzer = None\n",
                "            print(f\"Warning: Could not load sentiment analyzer: {e}\")\n",
                "\n",
                "        self.jargon_terms = {\n",
                "            'utilize', 'paradigm', 'synergy', 'leverage', 'optimize', 'streamline',\n",
                "            'methodology', 'framework', 'infrastructure', 'scalable', 'robust',\n",
                "            'innovative', 'cutting-edge', 'state-of-the-art', 'holistic', 'comprehensive'\n",
                "        }\n",
                "        \n",
                "        self.first_principles_indicators = {\n",
                "            'fundamental_starters': [\n",
                "                'imagine', 'think of', 'picture', 'let\\'s start with', 'at its core',\n",
                "                'fundamentally', 'basically', 'essentially', 'from the beginning',\n",
                "                'the basic idea', 'the foundation'\n",
                "            ],\n",
                "            'analogy_patterns': [\n",
                "                'like', 'similar', 'imagine', 'think of', 'picture', 'as if',\n",
                "                'it\\'s like when', 'just like', 'similar to', 'comparable to'\n",
                "            ],\n",
                "            'step_indicators': [\n",
                "                'first', 'second', 'third', 'next', 'then', 'after that',\n",
                "                'step by step', 'one by one', 'gradually', 'building up'\n",
                "            ],\n",
                "            'engagement_patterns': [\n",
                "                'does this', 'do you see', 'can you picture', 'have you noticed',\n",
                "                'does this help', 'make sense', 'clear now', 'understand how'\n",
                "            ]\n",
                "        }\n",
                "\n",
                "    def evaluate_analogy_quality(self, response: str) -> float:\n",
                "        \"\"\"Evaluate the quality of analogies in the response.\"\"\"\n",
                "        score = 0.0\n",
                "        response_lower = response.lower()\n",
                "        \n",
                "        analogy_count = 0\n",
                "        for pattern in self.first_principles_indicators['analogy_patterns']:\n",
                "            analogy_count += len(re.findall(rf'\\\\b{pattern}\\\\b', response_lower))\n",
                "\n",
                "        if analogy_count > 0:\n",
                "            score += 0.3\n",
                "        \n",
                "        concrete_examples = [\n",
                "            'ball', 'car', 'house', 'water', 'air', 'food', 'game', 'toy',\n",
                "            'bicycle', 'seesaw', 'playground', 'kitchen', 'garden', 'road',\n",
                "            'bridge', 'ladder', 'puzzle', 'painting', 'story', 'movie'\n",
                "        ]\n",
                "\n",
                "        concrete_count = sum(1 for word in response_lower.split() if word in concrete_examples)\n",
                "        score += min(0.4, concrete_count * 0.1)\n",
                "\n",
                "        sensory_words = [\n",
                "            'see', 'feel', 'hear', 'touch', 'taste', 'smell', 'warm', 'cold',\n",
                "            'bright', 'dark', 'smooth', 'rough', 'loud', 'quiet'\n",
                "        ]\n",
                "\n",
                "        sensory_count = sum(1 for word in response_lower.split() if word in sensory_words)\n",
                "        score += min(0.3, sensory_count * 0.05)\n",
                "\n",
                "        return min(1.0, score)\n",
                "\n",
                "    def evaluate_step_by_step_reasoning(self, response: str) -> float:\n",
                "        \"\"\"Evaluate if the explanation follows a logical, step-by-step progression\"\"\"\n",
                "        score = 0.0\n",
                "        response_lower = response.lower()\n",
                "        \n",
                "        step_indicators = self.first_principles_indicators['step_indicators']\n",
                "        step_count = sum(1 for indicator in step_indicators \n",
                "                        if indicator in response_lower)\n",
                "        \n",
                "        score += min(0.4, step_count * 0.1)\n",
                "        \n",
                "        connectors = [\n",
                "            'because', 'so', 'therefore', 'as a result', 'this means',\n",
                "            'which leads to', 'causing', 'resulting in', 'this is why'\n",
                "        ]\n",
                "        connector_count = sum(1 for connector in connectors \n",
                "                            if connector in response_lower)\n",
                "        \n",
                "        score += min(0.3, connector_count * 0.1)\n",
                "        \n",
                "        sentences = nltk.sent_tokenize(response)\n",
                "        if len(sentences) >= 3:\n",
                "            early_sentence_length = sum(len(s.split()) for s in sentences[:len(sentences)//2])\n",
                "            later_sentence_length = sum(len(s.split()) for s in sentences[len(sentences)//2:])\n",
                "            \n",
                "            if later_sentence_length > early_sentence_length:\n",
                "                score += 0.3\n",
                "        \n",
                "        return min(1.0, score)\n",
                "\n",
                "    def evaluate_fundamental_concepts(self, response: str) -> float:\n",
                "        \"\"\"Evaluate if the explanation addresses fundamental concepts\"\"\"\n",
                "        score = 0.0\n",
                "        response_lower = response.lower()\n",
                "        \n",
                "        fundamental_starters = self.first_principles_indicators['fundamental_starters']\n",
                "        starter_count = sum(1 for starter in fundamental_starters \n",
                "                          if starter in response_lower)\n",
                "        \n",
                "        score += min(0.4, starter_count * 0.2)\n",
                "        \n",
                "        why_patterns = ['why', 'reason', 'cause', 'because', 'due to', 'leads to']\n",
                "        why_count = sum(1 for pattern in why_patterns \n",
                "                       if pattern in response_lower)\n",
                "        \n",
                "        score += min(0.3, why_count * 0.05)\n",
                "        \n",
                "        building_blocks = [\n",
                "            'basic', 'fundamental', 'core', 'essential', 'underlying',\n",
                "            'foundation', 'principle', 'rule', 'law', 'truth'\n",
                "        ]\n",
                "        building_count = sum(1 for block in building_blocks \n",
                "                           if block in response_lower)\n",
                "        \n",
                "        score += min(0.3, building_count * 0.1)\n",
                "        \n",
                "        return min(1.0, score)\n",
                "\n",
                "    def evaluate_engagement(self, response: str) -> float:\n",
                "        \"\"\"Evaluate how engaging and interactive the explanation is\"\"\"\n",
                "        score = 0.0\n",
                "        response_lower = response.lower()\n",
                "        \n",
                "        engagement_patterns = self.first_principles_indicators['engagement_patterns']\n",
                "        engagement_count = sum(1 for pattern in engagement_patterns \n",
                "                             if pattern in response_lower)\n",
                "        \n",
                "        score += min(0.4, engagement_count * 0.1)\n",
                "        \n",
                "        question_count = response.count('?')\n",
                "        score += min(0.3, question_count * 0.1)\n",
                "        \n",
                "        if self.sentiment_analyzer:\n",
                "            try:\n",
                "                sentiment = self.sentiment_analyzer(response[:512])\n",
                "                if sentiment[0]['label'] == 'POSITIVE':\n",
                "                    score += 0.3\n",
                "            except:\n",
                "                pass\n",
                "        \n",
                "        return min(1.0, score)\n",
                "\n",
                "    def evaluate_clarity(self, response: str) -> float:\n",
                "        \"\"\"Evaluate clarity using readability metrics\"\"\"\n",
                "        score = 0.0\n",
                "        \n",
                "        try:\n",
                "            flesch_score = flesch_reading_ease(response)\n",
                "            if flesch_score >= 60:\n",
                "                score += 0.4\n",
                "            elif flesch_score >= 50:\n",
                "                score += 0.3\n",
                "            elif flesch_score >= 40:\n",
                "                score += 0.2\n",
                "            else:\n",
                "                score += 0.1\n",
                "        except:\n",
                "            score += 0.2\n",
                "        \n",
                "        sentences = nltk.sent_tokenize(response)\n",
                "        if sentences:\n",
                "            avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)\n",
                "            if 10 <= avg_sentence_length <= 20:\n",
                "                score += 0.3\n",
                "            elif 8 <= avg_sentence_length <= 25:\n",
                "                score += 0.2\n",
                "            else:\n",
                "                score += 0.1\n",
                "        \n",
                "        simple_words = ['the', 'a', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for']\n",
                "        word_count = len(response.split())\n",
                "        simple_word_ratio = sum(1 for word in response.lower().split() \n",
                "                               if word in simple_words) / max(word_count, 1)\n",
                "        \n",
                "        if simple_word_ratio >= 0.3:\n",
                "            score += 0.3\n",
                "        \n",
                "        return min(1.0, score)\n",
                "\n",
                "    def evaluate_completeness(self, response: str) -> float:\n",
                "        \"\"\"Evaluate if the explanation is complete\"\"\"\n",
                "        score = 0.0\n",
                "        \n",
                "        word_count = len(response.split())\n",
                "        if 100 <= word_count <= 300:\n",
                "            score += 0.5\n",
                "        elif 60 <= word_count <= 400:\n",
                "            score += 0.3\n",
                "        else:\n",
                "            score += 0.1\n",
                "        \n",
                "        conclusion_indicators = [\n",
                "            'so', 'therefore', 'in summary', 'to summarize', 'overall',\n",
                "            'does this help', 'now you can see', 'this explains'\n",
                "        ]\n",
                "        \n",
                "        response_lower = response.lower()\n",
                "        for indicator in conclusion_indicators:\n",
                "            if indicator in response_lower:\n",
                "                score += 0.3\n",
                "                break\n",
                "        \n",
                "        example_indicators = ['example', 'for instance', 'like when', 'such as']\n",
                "        example_count = sum(1 for indicator in example_indicators \n",
                "                          if indicator in response_lower)\n",
                "        \n",
                "        score += min(0.2, example_count * 0.1)\n",
                "        \n",
                "        return min(1.0, score)\n",
                "\n",
                "    def evaluate_jargon_avoidance(self, response: str) -> float:\n",
                "        \"\"\"Penalize use of jargon\"\"\"\n",
                "        words = response.lower().split()\n",
                "        jargon_count = sum(1 for word in words if word in self.jargon_terms)\n",
                "        total_words = len(words)\n",
                "        \n",
                "        if total_words == 0:\n",
                "            return 1.0\n",
                "        \n",
                "        jargon_ratio = jargon_count / total_words\n",
                "\n",
                "        if jargon_ratio == 0:\n",
                "            return 1.0\n",
                "        elif jargon_ratio <= 0.02:\n",
                "            return 0.8\n",
                "        elif jargon_ratio <= 0.05:\n",
                "            return 0.6\n",
                "        else:\n",
                "            return 0.3\n",
                "\n",
                "    def compute_reward(self, response: str, context: str = None) -> Dict[str, float]:\n",
                "        \"\"\"Compute the overall reward score\"\"\"\n",
                "        scores = {\n",
                "            'analogy_quality': self.evaluate_analogy_quality(response),\n",
                "            'step_by_step': self.evaluate_step_by_step_reasoning(response),\n",
                "            'fundamental_concepts': self.evaluate_fundamental_concepts(response),\n",
                "            'engagement': self.evaluate_engagement(response),\n",
                "            'clarity': self.evaluate_clarity(response),\n",
                "            'completeness': self.evaluate_completeness(response),\n",
                "            'avoid_jargon': self.evaluate_jargon_avoidance(response)\n",
                "        }\n",
                "        \n",
                "        total_score = sum(scores[key] * self.weights[key] for key in scores.keys())\n",
                "        \n",
                "        scores['total'] = total_score\n",
                "        scores['normalized'] = min(1.0, max(0.0, total_score))\n",
                "        \n",
                "        return scores\n",
                "\n",
                "# Initialize the reward function\n",
                "reward_evaluator = FirstPrinciplesRewardFunction()\n",
                "\n",
                "def reward_opening_hook(response: str, context: str = None) -> float:\n",
                "    \"\"\"Main reward function for GRPO training.\"\"\"\n",
                "    scores = reward_evaluator.compute_reward(response, context)\n",
                "    return scores['normalized']\n",
                "\n",
                "def detailed_reward_analysis(response: str, context: str = None) -> Dict[str, Any]:\n",
                "    \"\"\"Returns detailed breakdown of reward components\"\"\"\n",
                "    return reward_evaluator.compute_reward(response, context)\n",
                "\n",
                "def reward_with_feedback(response: str, context: str = None) -> Tuple[float, str]:\n",
                "    \"\"\"Returns reward score and human-readable feedback\"\"\"\n",
                "    scores = reward_evaluator.compute_reward(response, context)\n",
                "    \n",
                "    feedback_parts = []\n",
                "    \n",
                "    if scores['analogy_quality'] < 0.5:\n",
                "        feedback_parts.append(\"Consider adding more concrete analogies or examples.\")\n",
                "    \n",
                "    if scores['step_by_step'] < 0.5:\n",
                "        feedback_parts.append(\"Try breaking down the explanation into clearer steps.\")\n",
                "    \n",
                "    if scores['fundamental_concepts'] < 0.5:\n",
                "        feedback_parts.append(\"Focus more on the fundamental 'why' and underlying principles.\")\n",
                "    \n",
                "    if scores['engagement'] < 0.5:\n",
                "        feedback_parts.append(\"Make the explanation more engaging with questions.\")\n",
                "    \n",
                "    if scores['clarity'] < 0.5:\n",
                "        feedback_parts.append(\"Simplify the language and sentence structure.\")\n",
                "    \n",
                "    if scores['completeness'] < 0.5:\n",
                "        feedback_parts.append(\"Provide a more complete explanation with examples.\")\n",
                "    \n",
                "    if scores['avoid_jargon'] < 0.7:\n",
                "        feedback_parts.append(\"Avoid technical jargon and use simpler language.\")\n",
                "\n",
                "    feedback = \" \".join(feedback_parts) if feedback_parts else \"Great first principles explanation!\"\n",
                "    \n",
                "    return scores['normalized'], feedback\n",
                "\n",
                "print(\"‚úÖ Reward function implemented successfully!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## ‚öôÔ∏è Configuration and Model Setup\n",
                "Set up the training configuration and load the model with LoRA\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "from transformers import (\n",
                "    AutoTokenizer, \n",
                "    BitsAndBytesConfig, \n",
                "    TrainingArguments,\n",
                "    Qwen2VLForConditionalGeneration,\n",
                "    prepare_model_for_kbit_training\n",
                ")\n",
                "from trl import GRPOTrainer, GRPOConfig\n",
                "from peft import LoraConfig, get_peft_model\n",
                "from datasets import Dataset\n",
                "import json\n",
                "\n",
                "# Model and dataset configuration\n",
                "model_name = \"KhushalM/Qwen2-VL-2B-Instruct-SFT\"\n",
                "output_dir = \"./grpo_results\"\n",
                "hub_model_id = \"KhushalM/Qwen2-VL-2B-Instruct-GRPO-FirstPrinciples\"\n",
                "\n",
                "# Check device\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Initialize W&B\n",
                "wandb.init(\n",
                "    project=\"qwen2-vl-2b-instruct-grpo-first-principles\",\n",
                "    config={\n",
                "        \"model\": model_name,\n",
                "        \"task\": \"First Principles Explanations\",\n",
                "        \"reward_strategy\": \"Multi-component First Principles Reward\",\n",
                "        \"lora_r\": 32,\n",
                "        \"lora_alpha\": 64,\n",
                "        \"learning_rate\": 1e-5,\n",
                "        \"batch_size\": 2,\n",
                "        \"gradient_accumulation_steps\": 4,\n",
                "        \"num_train_epochs\": 3,\n",
                "        \"reward_components\": {\n",
                "            \"analogy_quality\": 0.20,\n",
                "            \"step_by_step\": 0.15,\n",
                "            \"fundamental_concepts\": 0.20,\n",
                "            \"engagement\": 0.15,\n",
                "            \"clarity\": 0.15,\n",
                "            \"completeness\": 0.10,\n",
                "            \"avoid_jargon\": 0.05\n",
                "        }\n",
                "    }\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Configuration completed!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quantization configuration for efficient training\n",
                "quantization_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "print(\"Loading model...\")\n",
                "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=quantization_config,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "model = model.to(device)\n",
                "\n",
                "# Load tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# Prepare model for training\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "model.config.use_cache = False\n",
                "print(\"Model prepared for kbit training\")\n",
                "\n",
                "# LoRA Setup\n",
                "peft_config = LoraConfig(\n",
                "    r=32,\n",
                "    lora_alpha=64,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\"\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, peft_config)\n",
                "print(\"Model converted to LoRA\")\n",
                "print(f\"Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
                "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "\n",
                "print(\"‚úÖ Model loaded and configured successfully!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## üìÅ Dataset Upload Options\n",
                "Choose one of the following methods to upload your dataset\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 1: Upload file directly to Colab\n",
                "from google.colab import files\n",
                "import json\n",
                "import os\n",
                "\n",
                "def upload_dataset_file():\n",
                "    \"\"\"Upload dataset file directly to Colab\"\"\"\n",
                "    print(\"üì§ Upload your dataset file (JSON format)\")\n",
                "    print(\"Expected format: List of dictionaries with 'messages' key\")\n",
                "    print(\"Each message should have 'role' and 'content' fields\")\n",
                "    \n",
                "    uploaded = files.upload()\n",
                "    \n",
                "    if uploaded:\n",
                "        filename = list(uploaded.keys())[0]\n",
                "        print(f\"‚úÖ File uploaded: {filename}\")\n",
                "        \n",
                "        # Validate file format\n",
                "        try:\n",
                "            with open(filename, 'r') as f:\n",
                "                data = json.load(f)\n",
                "            \n",
                "            if isinstance(data, list) and len(data) > 0:\n",
                "                if 'messages' in data[0]:\n",
                "                    print(f\"‚úÖ Dataset format validated!\")\n",
                "                    print(f\"üìä Dataset contains {len(data)} examples\")\n",
                "                    return filename\n",
                "                else:\n",
                "                    print(\"‚ùå Invalid format: Each item should have 'messages' key\")\n",
                "            else:\n",
                "                print(\"‚ùå Invalid format: Dataset should be a list of dictionaries\")\n",
                "                \n",
                "        except json.JSONDecodeError:\n",
                "            print(\"‚ùå Invalid JSON format\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Error validating file: {e}\")\n",
                "    \n",
                "    return None\n",
                "\n",
                "# Uncomment the line below to upload a file\n",
                "# dataset_filename = upload_dataset_file()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 2: Download from Google Drive\n",
                "def download_from_drive(drive_path, local_filename=\"dataset.json\"):\n",
                "    \"\"\"Download dataset from Google Drive\"\"\"\n",
                "    print(f\"üì• Downloading from Google Drive: {drive_path}\")\n",
                "    \n",
                "    try:\n",
                "        # Copy file from Google Drive to local Colab storage\n",
                "        import shutil\n",
                "        shutil.copy(drive_path, local_filename)\n",
                "        \n",
                "        # Validate the downloaded file\n",
                "        with open(local_filename, 'r') as f:\n",
                "            data = json.load(f)\n",
                "        \n",
                "        if isinstance(data, list) and len(data) > 0 and 'messages' in data[0]:\n",
                "            print(f\"‚úÖ Dataset downloaded and validated!\")\n",
                "            print(f\"üìä Dataset contains {len(data)} examples\")\n",
                "            return local_filename\n",
                "        else:\n",
                "            print(\"‚ùå Invalid dataset format\")\n",
                "            \n",
                "    except FileNotFoundError:\n",
                "        print(f\"‚ùå File not found: {drive_path}\")\n",
                "        print(\"Make sure Google Drive is mounted and the path is correct\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error downloading file: {e}\")\n",
                "    \n",
                "    return None\n",
                "\n",
                "# Example usage (uncomment and modify path):\n",
                "# drive_dataset_path = \"/content/drive/MyDrive/your_folder/structured_dataset.json\"\n",
                "# dataset_filename = download_from_drive(drive_dataset_path)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 4: Load from Hugging Face Hub\n",
                "from datasets import load_dataset\n",
                "\n",
                "def load_from_huggingface(dataset_name, split=\"train\", config_name=None):\n",
                "    \"\"\"Load dataset from Hugging Face Hub\"\"\"\n",
                "    print(f\"ü§ó Loading from Hugging Face: {dataset_name}\")\n",
                "    \n",
                "    try:\n",
                "        # Load dataset from HF Hub\n",
                "        if config_name:\n",
                "            dataset = load_dataset(dataset_name, config_name, split=split)\n",
                "        else:\n",
                "            dataset = load_dataset(dataset_name, split=split)\n",
                "        \n",
                "        # Convert to expected format if needed\n",
                "        data = []\n",
                "        for item in dataset:\n",
                "            # Check if already in correct format\n",
                "            if 'messages' in item:\n",
                "                data.append(item)\n",
                "            # Convert common formats\n",
                "            elif 'conversations' in item:\n",
                "                data.append({'messages': item['conversations']})\n",
                "            elif 'prompt' in item and 'response' in item:\n",
                "                messages = [\n",
                "                    {\"role\": \"user\", \"content\": item['prompt']},\n",
                "                    {\"role\": \"assistant\", \"content\": item['response']}\n",
                "                ]\n",
                "                data.append({'messages': messages})\n",
                "            else:\n",
                "                print(f\"‚ö†Ô∏è Unknown format for item: {list(item.keys())}\")\n",
                "                continue\n",
                "        \n",
                "        if data:\n",
                "            # Save as JSON file\n",
                "            local_filename = \"hf_dataset.json\"\n",
                "            with open(local_filename, 'w') as f:\n",
                "                json.dump(data, f, indent=2)\n",
                "            \n",
                "            print(f\"‚úÖ Dataset loaded and converted!\")\n",
                "            print(f\"üìä Dataset contains {len(data)} examples\")\n",
                "            return local_filename\n",
                "        else:\n",
                "            print(\"‚ùå No valid data found in dataset\")\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error loading from Hugging Face: {e}\")\n",
                "    \n",
                "    return None\n",
                "\n",
                "# Example usage (uncomment and provide dataset name):\n",
                "# hf_dataset_name = \"your_username/your_dataset\"\n",
                "# dataset_filename = load_from_huggingface(hf_dataset_name)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset loading - Choose your method and uncomment the appropriate line\n",
                "\n",
                "# Method 1: Upload file directly\n",
                "# dataset_filename = upload_dataset_file()\n",
                "\n",
                "# Method 2: Load from Google Drive (update the path)\n",
                "# drive_path = \"/content/drive/MyDrive/your_folder/structured_dataset.json\"\n",
                "# dataset_filename = download_from_drive(drive_path)\n",
                "\n",
                "# Method 4: Load from Hugging Face Hub\n",
                "# hf_dataset_name = \"your_username/your_dataset\"\n",
                "# dataset_filename = load_from_huggingface(hf_dataset_name)\n",
                "\n",
                "# Verify dataset was loaded\n",
                "if dataset_filename and os.path.exists(dataset_filename):\n",
                "    print(f\"‚úÖ Dataset file ready: {dataset_filename}\")\n",
                "    \n",
                "    # Preview the dataset structure\n",
                "    with open(dataset_filename, 'r') as f:\n",
                "        sample_data = json.load(f)\n",
                "    \n",
                "    print(f\"\\nüìã Dataset Preview:\")\n",
                "    print(f\"Total examples: {len(sample_data)}\")\n",
                "    print(f\"First example structure:\")\n",
                "    if len(sample_data) > 0:\n",
                "        example = sample_data[0]\n",
                "        print(f\"  Keys: {list(example.keys())}\")\n",
                "        if 'messages' in example:\n",
                "            print(f\"  Messages count: {len(example['messages'])}\")\n",
                "            print(f\"  Message roles: {[msg.get('role', 'unknown') for msg in example['messages']]}\")\n",
                "            \n",
                "            # Show first user message\n",
                "            user_msg = next((msg for msg in example['messages'] if msg.get('role') == 'user'), None)\n",
                "            if user_msg:\n",
                "                preview = user_msg['content'][:100] + \"...\" if len(user_msg['content']) > 100 else user_msg['content']\n",
                "                print(f\"  Sample question: \\\"{preview}\\\"\")\n",
                "else:\n",
                "    print(\"‚ùå No dataset loaded. Please use one of the upload methods above.\")\n",
                "    dataset_filename = None\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and process the dataset for training\n",
                "if dataset_filename:\n",
                "    print(\"üìÇ Processing dataset for training...\")\n",
                "    \n",
                "    try:\n",
                "        # Load the dataset\n",
                "        with open(dataset_filename, \"r\") as f:\n",
                "            dataset_data = json.load(f)\n",
                "        \n",
                "        # Convert to HuggingFace Dataset format\n",
                "        dataset = Dataset.from_list(dataset_data)\n",
                "        dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
                "        train_dataset = dataset[\"train\"]\n",
                "        val_dataset = dataset[\"test\"]\n",
                "        \n",
                "        print(f\"‚úÖ Dataset processed successfully!\")\n",
                "        print(f\"üìä Train dataset size: {len(train_dataset)}\")\n",
                "        print(f\"üìä Validation dataset size: {len(val_dataset)}\")\n",
                "        \n",
                "        # Dataset formatting function\n",
                "        def format_dataset(sample):\n",
                "            \"\"\"Format the sample for training\"\"\"\n",
                "            messages = sample[\"messages\"]\n",
                "            # Extract the user question for context\n",
                "            user_message = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\"), \"\")\n",
                "            \n",
                "            # Format as instruction-response pair\n",
                "            prompt = tokenizer.apply_chat_template(\n",
                "                messages[:-1],  # All messages except the last (assistant) one\n",
                "                tokenize=False,\n",
                "                add_generation_prompt=True\n",
                "            )\n",
                "            \n",
                "            return {\n",
                "                \"prompt\": prompt,\n",
                "                \"response\": messages[-1][\"content\"],  # Assistant's response\n",
                "                \"context\": user_message\n",
                "            }\n",
                "\n",
                "        # Format datasets\n",
                "        print(\"üîÑ Formatting datasets for GRPO training...\")\n",
                "        train_formatted = train_dataset.map(format_dataset)\n",
                "        val_formatted = val_dataset.map(format_dataset)\n",
                "        \n",
                "        print(\"‚úÖ Dataset formatting completed!\")\n",
                "        print(\"üéØ Ready for GRPO training!\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error processing dataset: {e}\")\n",
                "        print(\"Please check your dataset format and try again.\")\n",
                "        train_formatted = None\n",
                "        val_formatted = None\n",
                "        \n",
                "else:\n",
                "    print(\"‚ùå No dataset available. Please load a dataset first.\")\n",
                "    train_formatted = None\n",
                "    val_formatted = None\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## üìä Dataset Loading and Preprocessing\n",
                "Load and format the dataset for GRPO training\n",
                "\n",
                "**Note:** Update the dataset path to point to your dataset file. You can either:\n",
                "1. Upload your dataset to Google Drive and mount it\n",
                "2. Upload directly to Colab files\n",
                "3. Use a dataset from Hugging Face Hub\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset configuration - UPDATE THESE PATHS AS NEEDED\n",
                "# Option 1: From Google Drive (update path)\n",
                "# dataset_path = \"/content/drive/MyDrive/your_dataset_path/structured_dataset.json\"\n",
                "\n",
                "# Option 2: From uploaded file in Colab\n",
                "# dataset_path = \"/content/structured_dataset.json\"\n",
                "\n",
                "# Option 3: Create a sample dataset for testing (remove this and use real data)\n",
                "sample_dataset = [\n",
                "    {\n",
                "        \"messages\": [\n",
                "            {\n",
                "                \"role\": \"system\",\n",
                "                \"content\": \"You are an expert educator who explains concepts from first principles like Richard Feynman. Start with fundamental truths, use simple analogies, and avoid jargon.\"\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"user\", \n",
                "                \"content\": \"Why do objects fall to the ground when dropped?\"\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"assistant\",\n",
                "                \"content\": \"Imagine you have a stretched rubber sheet and you place a heavy ball in the middle. The sheet bends downwards, right? Now, if you roll a smaller ball nearby, it will start rolling toward the heavier ball because of the dip. This is a simple way to picture how gravity works. Gravity is like the Earth making a 'dip' in space that pulls things toward it. When you let go of an object, it falls because the Earth is pulling it toward its center, similar to how the heavy ball makes the rubber sheet dip. The bigger the object (like Earth), the stronger the pull. Does this help you see why objects fall when dropped?\"\n",
                "            }\n",
                "        ]\n",
                "    }\n",
                "] * 50  # Repeat for demo purposes\n",
                "\n",
                "# For real training, uncomment one of the options above and comment out the sample\n",
                "print(\"Using sample dataset for demonstration. Replace with your actual dataset!\")\n",
                "\n",
                "# Load dataset\n",
                "try:\n",
                "    # For real data: \n",
                "    # with open(dataset_path, \"r\") as f:\n",
                "    #     dataset_data = json.load(f)\n",
                "    \n",
                "    # For demo:\n",
                "    dataset_data = sample_dataset\n",
                "    \n",
                "    dataset = Dataset.from_list(dataset_data)\n",
                "    dataset = dataset.train_test_split(test_size=0.1)\n",
                "    train_dataset = dataset[\"train\"]\n",
                "    val_dataset = dataset[\"test\"]\n",
                "    \n",
                "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
                "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
                "    print(f\"Test dataset size: {len(val_dataset)}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error loading dataset: {e}\")\n",
                "    print(\"Please check your dataset path and format.\")\n",
                "\n",
                "# Dataset formatting function\n",
                "def format_dataset(sample):\n",
                "    \"\"\"Format the sample for training\"\"\"\n",
                "    messages = sample[\"messages\"]\n",
                "    # Extract the user question for context\n",
                "    user_message = next((msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\"), \"\")\n",
                "    \n",
                "    # Format as instruction-response pair\n",
                "    prompt = tokenizer.apply_chat_template(\n",
                "        messages[:-1],  # All messages except the last (assistant) one\n",
                "        tokenize=False,\n",
                "        add_generation_prompt=True\n",
                "    )\n",
                "    \n",
                "    return {\n",
                "        \"prompt\": prompt,\n",
                "        \"response\": messages[-1][\"content\"],  # Assistant's response\n",
                "        \"context\": user_message\n",
                "    }\n",
                "\n",
                "# Format datasets\n",
                "print(\"Formatting datasets...\")\n",
                "train_formatted = train_dataset.map(format_dataset)\n",
                "val_formatted = val_dataset.map(format_dataset)\n",
                "print(\"‚úÖ Dataset formatting completed!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## üöÄ GRPO Training Setup and Execution\n",
                "Configure the GRPO trainer and start training\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reward function wrapper for GRPO\n",
                "def compute_reward(responses, contexts=None):\n",
                "    \"\"\"\n",
                "    Compute the reward for the response using our custom reward function\n",
                "    \"\"\"\n",
                "    if isinstance(responses, str):\n",
                "        responses = [responses]\n",
                "    if contexts is None:\n",
                "        contexts = [None] * len(responses)\n",
                "    elif isinstance(contexts, str):\n",
                "        contexts = [contexts]\n",
                "    \n",
                "    rewards = []\n",
                "    for response, context in zip(responses, contexts):\n",
                "        score, feedback = reward_with_feedback(response, context)\n",
                "        rewards.append(score)\n",
                "    return rewards\n",
                "\n",
                "# GRPO Configuration\n",
                "grpo_config = GRPOConfig(\n",
                "    output_dir=output_dir,\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=2,\n",
                "    per_device_eval_batch_size=2,\n",
                "    gradient_accumulation_steps=8,\n",
                "    learning_rate=1e-5,\n",
                "    lr_scheduler_type=\"cosine\",\n",
                "    warmup_ratio=0.1,\n",
                "    logging_steps=10,\n",
                "    eval_steps=50,\n",
                "    save_steps=50,\n",
                "    evaluation_strategy=\"steps\",\n",
                "    save_strategy=\"steps\",\n",
                "    save_total_limit=3,\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"eval_reward\",\n",
                "    greater_is_better=True,\n",
                "    report_to=\"wandb\",\n",
                "    push_to_hub=True,\n",
                "    hub_model_id=hub_model_id,\n",
                "    hub_strategy=\"every_save\",\n",
                "    dataloader_num_workers=2,\n",
                "    remove_unused_columns=False,\n",
                "    # GRPO specific parameters\n",
                "    max_new_tokens=1024,\n",
                "    num_generations=4,\n",
                "    temperature=0.7,\n",
                "    kl_penalty=\"kl\",\n",
                "    kl_coef=0.05,\n",
                "    reward_model_path=None,\n",
                "    bf16=True if device == \"cuda\" else False,\n",
                ")\n",
                "\n",
                "print(\"‚úÖ GRPO configuration completed!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize GRPO Trainer\n",
                "if train_formatted is not None and val_formatted is not None:\n",
                "    print(\"Initializing GRPO Trainer...\")\n",
                "    trainer = GRPOTrainer(\n",
                "        model=model,\n",
                "        tokenizer=tokenizer,\n",
                "        args=grpo_config,\n",
                "        train_dataset=train_formatted,\n",
                "        eval_dataset=val_formatted,\n",
                "        reward_function=compute_reward,\n",
                "        data_collator=None,\n",
                "    )\n",
                "\n",
                "    print(\"‚úÖ GRPO Trainer initialized successfully!\")\n",
                "    print(f\"Number of training examples: {len(train_formatted)}\")\n",
                "    print(f\"Number of validation examples: {len(val_formatted)}\")\n",
                "else:\n",
                "    print(\"‚ùå Cannot initialize trainer: No dataset available.\")\n",
                "    print(\"Please load a dataset first using one of the upload methods above.\")\n",
                "    trainer = None\n",
                "\n",
                "# Custom callback for detailed logging\n",
                "class RewardLoggingCallback:\n",
                "    def __init__(self):\n",
                "        self.step_count = 0\n",
                "    \n",
                "    def on_log(self, logs):\n",
                "        \"\"\"Log detailed reward analysis periodically\"\"\"\n",
                "        if self.step_count % 50 == 0:  # Every 50 steps\n",
                "            # Sample a few responses for detailed analysis\n",
                "            sample_responses = logs.get('sample_responses', [])\n",
                "            if sample_responses:\n",
                "                for i, response in enumerate(sample_responses[:3]):  # First 3 samples\n",
                "                    scores = detailed_reward_analysis(response)\n",
                "                    wandb.log({\n",
                "                        f\"detailed_reward_sample_{i}/analogy_quality\": scores['analogy_quality'],\n",
                "                        f\"detailed_reward_sample_{i}/step_by_step\": scores['step_by_step'],\n",
                "                        f\"detailed_reward_sample_{i}/fundamental_concepts\": scores['fundamental_concepts'],\n",
                "                        f\"detailed_reward_sample_{i}/engagement\": scores['engagement'],\n",
                "                        f\"detailed_reward_sample_{i}/clarity\": scores['clarity'],\n",
                "                        f\"detailed_reward_sample_{i}/completeness\": scores['completeness'],\n",
                "                        f\"detailed_reward_sample_{i}/avoid_jargon\": scores['avoid_jargon'],\n",
                "                        f\"detailed_reward_sample_{i}/total\": scores['total'],\n",
                "                    })\n",
                "        self.step_count += 1\n",
                "\n",
                "# Add callback\n",
                "reward_callback = RewardLoggingCallback()\n",
                "\n",
                "print(\"üìã Training setup completed! Ready to start training...\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start GRPO training\n",
                "if trainer is not None:\n",
                "    print(\"üöÄ Starting GRPO training...\")\n",
                "    print(\"This will optimize for first principles explanations using our custom reward function\")\n",
                "    print(\"Training may take several hours depending on your GPU and dataset size...\")\n",
                "\n",
                "    try:\n",
                "        trainer.train()\n",
                "        print(\"‚úÖ Training completed successfully!\")\n",
                "        \n",
                "        # Save the final model\n",
                "        print(\"üíæ Saving final model...\")\n",
                "        trainer.save_model(os.path.join(output_dir, \"final_model\"))\n",
                "        tokenizer.save_pretrained(os.path.join(output_dir, \"final_model\"))\n",
                "        \n",
                "        # Push to hub if configured\n",
                "        try:\n",
                "            trainer.push_to_hub(commit_message=\"GRPO fine-tuned model for first principles explanations\")\n",
                "            print(\"‚úÖ Model pushed to Hugging Face Hub successfully!\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è Could not push to hub: {e}\")\n",
                "        \n",
                "        print(\"üéâ Training process completed successfully!\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Training failed with error: {e}\")\n",
                "        print(\"Check the error message above for details.\")\n",
                "        raise e\n",
                "        \n",
                "else:\n",
                "    print(\"‚ùå Cannot start training: No trainer available.\")\n",
                "    print(\"Please make sure you have:\")\n",
                "    print(\"1. Loaded a dataset using one of the upload methods\")\n",
                "    print(\"2. Successfully initialized the model and trainer\")\n",
                "    print(\"3. Check for any error messages above\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## üß™ Test the Trained Model\n",
                "Test the model with sample prompts to see the improvement\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the trained model\n",
                "if trainer is not None and 'model' in locals():\n",
                "    print(\"üß™ Testing the trained model...\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è No trained model available for testing.\")\n",
                "    print(\"Please complete the training process first.\")\n",
                "\n",
                "# Test prompts\n",
                "test_prompts = [\n",
                "    \"Why do objects fall to the ground when dropped?\",\n",
                "    \"How does a microwave oven heat food?\",\n",
                "    \"Why is the sky blue?\",\n",
                "    \"How do airplanes fly?\",\n",
                "    \"What makes magnets work?\"\n",
                "]\n",
                "\n",
                "def test_model_response(prompt, max_length=300):\n",
                "    \"\"\"Generate and evaluate a response from the trained model\"\"\"\n",
                "    \n",
                "    test_messages = [\n",
                "        {\n",
                "            \"role\": \"system\", \n",
                "            \"content\": \"You are an expert educator who explains concepts from first principles like Richard Feynman. Start with fundamental truths, use simple analogies, and avoid jargon. Use a storytelling tone and follow a step by step explanation style.\"\n",
                "        },\n",
                "        {\n",
                "            \"role\": \"user\", \n",
                "            \"content\": prompt\n",
                "        }\n",
                "    ]\n",
                "    \n",
                "    formatted_test = tokenizer.apply_chat_template(\n",
                "        test_messages,\n",
                "        tokenize=False,\n",
                "        add_generation_prompt=True\n",
                "    )\n",
                "    \n",
                "    inputs = tokenizer(formatted_test, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_length,\n",
                "            temperature=0.7,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id,\n",
                "            eos_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    # Extract just the assistant's response\n",
                "    if \"<|im_start|>assistant\" in response:\n",
                "        assistant_response = response.split(\"<|im_start|>assistant\")[-1].strip()\n",
                "    else:\n",
                "        assistant_response = response.split(formatted_test)[-1].strip()\n",
                "    \n",
                "    return assistant_response\n",
                "\n",
                "# Test each prompt (only if model is available)\n",
                "if trainer is not None and 'model' in locals():\n",
                "    for i, prompt in enumerate(test_prompts):\n",
                "        print(f\"\\n{'='*60}\")\n",
                "        print(f\"TEST {i+1}: {prompt}\")\n",
                "        print('='*60)\n",
                "        \n",
                "        try:\n",
                "            response = test_model_response(prompt)\n",
                "            print(f\"ü§ñ Model Response:\\n{response}\")\n",
                "            \n",
                "            # Evaluate with reward function\n",
                "            reward_score, feedback = reward_with_feedback(response)\n",
                "            detailed_scores = detailed_reward_analysis(response)\n",
                "            \n",
                "            print(f\"\\nüìä Evaluation:\")\n",
                "            print(f\"Overall Reward Score: {reward_score:.3f}\")\n",
                "            print(f\"Feedback: {feedback}\")\n",
                "            print(f\"\\nDetailed Scores:\")\n",
                "            for component, score in detailed_scores.items():\n",
                "                if component not in ['total', 'normalized']:\n",
                "                    print(f\"  {component}: {score:.3f}\")\n",
                "            \n",
                "            # Log to W&B\n",
                "            wandb.log({\n",
                "                f\"test_prompt_{i+1}/reward_score\": reward_score,\n",
                "                f\"test_prompt_{i+1}/response_length\": len(response.split()),\n",
                "                f\"test_prompt_{i+1}/analogy_quality\": detailed_scores['analogy_quality'],\n",
                "                f\"test_prompt_{i+1}/clarity\": detailed_scores['clarity'],\n",
                "                f\"test_prompt_{i+1}/engagement\": detailed_scores['engagement'],\n",
                "            })\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"‚ùå Error testing prompt: {e}\")\n",
                "\n",
                "    print(f\"\\nüéØ Model testing completed!\")\n",
                "    print(\"Check your W&B dashboard for detailed metrics and training progress.\")\n",
                "else:\n",
                "    print(\"üîç Skipping model testing - no trained model available.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean up and finish\n",
                "print(\"üßπ Cleaning up...\")\n",
                "\n",
                "# Finish W&B run\n",
                "wandb.finish()\n",
                "\n",
                "# Clear GPU memory\n",
                "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
                "\n",
                "print(\"‚úÖ Cleanup completed!\")\n",
                "print(\"\\nüéâ GRPO Training Notebook Execution Complete! üéâ\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"1. Check your trained model in the output directory\")\n",
                "print(\"2. Review metrics on your W&B dashboard\") \n",
                "print(\"3. Test the model with your own prompts\")\n",
                "print(\"4. Consider fine-tuning hyperparameters for better results\")\n",
                "print(\"\\nHappy training! üöÄ\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

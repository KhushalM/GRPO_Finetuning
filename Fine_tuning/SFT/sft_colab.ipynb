{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Qwen2-VL Fine-tuning with LoRA on Google Colab\n",
        "\n",
        "This notebook fine-tunes the Qwen2-VL-2B-Instruct model using LoRA (Low-Rank Adaptation) on your first principles dataset.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Enable GPU runtime: Runtime → Change runtime type → Hardware accelerator → GPU\n",
        "- Upload your dataset file to Colab or mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets accelerate peft trl bitsandbytes wandb torch torchvision\n",
        "!pip install -q qwen-vl-utils\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 2. Authentication\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Login to Hugging Face and Weights & Biases\n",
        "from huggingface_hub import login\n",
        "import wandb\n",
        "\n",
        "# HuggingFace login\n",
        "login()\n",
        "\n",
        "# WandB login\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Upload Dataset\n",
        "\n",
        "Upload your `first_principles_dataset.json` file using the file upload widget below, or mount Google Drive if your dataset is stored there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Option 1: Upload dataset file\n",
        "print(\"Upload your first_principles_dataset.json file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name\n",
        "dataset_file = list(uploaded.keys())[0]\n",
        "print(f\"Dataset uploaded: {dataset_file}\")\n",
        "\n",
        "# Verify dataset format\n",
        "with open(dataset_file, 'r') as f:\n",
        "    data = json.load(f)\n",
        "    print(f\"Dataset contains {len(data)} examples\")\n",
        "    print(\"Sample entry:\", data[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 2: Mount Google Drive (alternative to file upload)\n",
        "# Uncomment the lines below if you prefer to use Google Drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# dataset_file = '/content/drive/MyDrive/path/to/your/first_principles_dataset.json'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Model and Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    BitsAndBytesConfig, \n",
        "    Qwen2VLForConditionalGeneration,\n",
        "    TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "output_dir = \"./qwen2-vl-sft-results\"\n",
        "hub_model_id = \"your-username/Qwen2-VL-2B-Instruct-SFT\"  # Change this to your desired model name\n",
        "\n",
        "# Initialize WandB\n",
        "wandb.init(\n",
        "    project=\"qwen2-vl-sft-colab\",\n",
        "    config={\n",
        "        \"model\": model_name,\n",
        "        \"dataset\": dataset_file,\n",
        "        \"lora_r\": 32,\n",
        "        \"batch_size\": 1,\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"platform\": \"Google Colab\"\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4-bit quantization configuration for GPU memory efficiency\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"Quantization config created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model with quantization\n",
        "print(\"Loading model...\")\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.config.use_cache = False\n",
        "print(\"Model prepared for k-bit training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(\"LoRA configuration applied\")\n",
        "print(f\"Trainable parameters: {model.num_parameters(only_trainable=True):,}\")\n",
        "print(f\"Total parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Dataset Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare dataset\n",
        "with open(dataset_file, 'r') as f:\n",
        "    dataset_json = json.load(f)\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "dataset = Dataset.from_list(dataset_json)\n",
        "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
        "print(\"Sample entry:\", dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset formatting function\n",
        "def format_dataset(sample):\n",
        "    \"\"\"Format the dataset for chat template\"\"\"\n",
        "    return tokenizer.apply_chat_template(\n",
        "        sample[\"messages\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "\n",
        "# Test the formatting function\n",
        "sample_formatted = format_dataset(dataset[0])\n",
        "print(\"Formatted sample (first 300 chars):\")\n",
        "print(sample_formatted[:300] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Training Configuration and Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    report_to=\"wandb\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_steps=10,\n",
        "    save_only_model=True,\n",
        "    log_on_each_node=True,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=hub_model_id,\n",
        "    max_length=2048,\n",
        "    packing=True,\n",
        "    dataset_text_field=\"messages\",\n",
        "    bf16=torch.cuda.is_available(),  # Use bf16 if GPU supports it\n",
        "    fp16=not torch.cuda.is_available() or not torch.cuda.is_bf16_supported(),\n",
        "    optim=\"adamw_torch\",\n",
        "    dataloader_num_workers=2,\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Total training steps: {len(dataset) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    formatting_func=format_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Trainer created successfully!\")\n",
        "print(f\"Number of training examples: {len(trainer.train_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "print(\"This may take 1-3 hours depending on your dataset size and GPU.\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7. Save and Test the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the final model\n",
        "final_model_path = f\"{output_dir}/final_model\"\n",
        "trainer.save_model(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "print(f\"Model saved to {final_model_path}\")\n",
        "print(\"Model files:\")\n",
        "!ls -la {final_model_path}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the fine-tuned model\n",
        "def test_model(prompt, max_length=200):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert educator who explains concepts from first principles like Richard Feynman. Start with fundamental truths, use simple analogies, and avoid jargon.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    \n",
        "    formatted_prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the assistant's response\n",
        "    assistant_response = response.split(\"<|im_start|>assistant\\n\")[-1]\n",
        "    return assistant_response\n",
        "\n",
        "# Test with a few prompts\n",
        "test_prompts = [\n",
        "    \"Why does ice float on water?\",\n",
        "    \"How do computers understand binary code?\",\n",
        "    \"What is gravity from a physics perspective?\"\n",
        "]\n",
        "\n",
        "print(\"Testing the fine-tuned model:\\n\")\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"Test {i}: {prompt}\")\n",
        "    response = test_model(prompt)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finish WandB run\n",
        "wandb.finish()\n",
        "print(\"Training complete! Check your WandB dashboard for training metrics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8. Download Your Model (Optional)\n",
        "\n",
        "If you want to download the trained model to your local machine:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a zip file of the trained model\n",
        "import shutil\n",
        "\n",
        "# Zip the final model\n",
        "shutil.make_archive('qwen2_vl_finetuned_model', 'zip', final_model_path)\n",
        "\n",
        "# Download the model\n",
        "from google.colab import files\n",
        "files.download('qwen2_vl_finetuned_model.zip')\n",
        "\n",
        "print(\"Model downloaded! You can now use this model locally.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 🎉 Training Complete!\n",
        "\n",
        "### What happened:\n",
        "- ✅ Loaded Qwen2-VL-2B-Instruct model with 4-bit quantization\n",
        "- ✅ Applied LoRA for efficient fine-tuning\n",
        "- ✅ Trained on your first principles dataset\n",
        "- ✅ Saved the model with adapters\n",
        "- ✅ Tested the fine-tuned model\n",
        "\n",
        "### Next steps:\n",
        "1. **Test more extensively**: Try various prompts to evaluate performance\n",
        "2. **Push to Hub**: Your model is automatically pushed to HuggingFace Hub\n",
        "3. **Use the model**: Load it in your applications or continue training\n",
        "4. **Iterate**: Adjust hyperparameters and retrain if needed\n",
        "\n",
        "### Model usage:\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
        "# Load your fine-tuned adapters\n",
        "model = PeftModel.from_pretrained(base_model, \"your-username/Qwen2-VL-2B-Instruct-SFT\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"your-username/Qwen2-VL-2B-Instruct-SFT\")\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
